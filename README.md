
# ‚ö° Proyecto Data Lake para Comercializaci√≥n de Energ√≠a

## üìå Descripci√≥n General
Este proyecto implementa una arquitectura completa de Data Lake para una empresa comercializadora de energ√≠a utilizando AWS. Se ingieren archivos CSV desde sistemas operativos (clientes, proveedores, transacciones), se procesan con AWS Glue, se almacenan en S3 en m√∫ltiples zonas, se consultan mediante Athena y Redshift, y se aplica gobernanza de datos con Lake Formation.

## üìê Arquitectura
El pipeline sigue una arquitectura en capas:

- **Zona Raw**: Archivos CSV originales ingeridos desde los sistemas fuente, particionados por fecha de carga.
- **Zona Procesada**: Datos convertidos a formato Parquet con validaci√≥n b√°sica y particionados por a√±o/mes/d√≠a.
- **Zona Curada**: Conjuntos de datos limpios y deduplicados optimizados para anal√≠tica.
- **Acceso Anal√≠tico**:
  - **Amazon Athena**: Consultas SQL directas sobre datos en S3.
  - **Amazon Redshift Spectrum**: Consultas externas sobre datos curados v√≠a Glue Catalog.

## üõ† Tecnolog√≠as Utilizadas
- `Amazon S3`: Almacenamiento del Data Lake
- `AWS Glue`: ETL (Jobs + Crawlers + Cat√°logo)
- `Amazon Athena`: Motor de consultas serverless sobre S3
- `Amazon Redshift`: Data Warehouse + Spectrum
- `AWS Lake Formation`: Gobernanza de datos y permisos
- `AWS CloudFormation`: Infraestructura como C√≥digo (IaC)
- `Python (boto3, pandas)`: Orquestaci√≥n y scripts anal√≠ticos

## üìÅ Estructura del Proyecto
```
AWS_AMARIS/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ curated/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md
‚îÇ   ‚îî‚îÄ‚îÄ technical_documentation.md
‚îú‚îÄ‚îÄ experimental/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ sql/
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îî‚îÄ‚îÄ environments/dev/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ python/
‚îÇ       ‚îú‚îÄ‚îÄ raw_to_processed.py
‚îÇ       ‚îú‚îÄ‚îÄ processed_to_curated.py
‚îÇ       ‚îî‚îÄ‚îÄ athena_queries.py
‚îú‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

## üì¶ Instalaci√≥n de dependencias

Antes de ejecutar los scripts, aseg√∫rate de instalar las librer√≠as necesarias con:

```bash
pip install -r requirements.txt
```

## üöÄ Gu√≠a de Despliegue

### Paso 1: Desplegar Infraestructura
```bash
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/s3.yaml --stack-name datalake-s3
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/lakeformation.yaml --stack-name datalake-lakeformation
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/glue.yaml --stack-name datalake-glue
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/redshift.yaml --stack-name datalake-redshift
```

### Paso 2: Ejecutar ETL Jobs
- `raw_to_processed.py`: Convierte CSV a Parquet y particiona por fecha.
- `processed_to_curated.py`: Limpia, deduplica y escribe datos curados.

### Paso 3: Actualizar Cat√°logo de Glue
```bash
# Ejecutar crawlers o configurar triggers luego de cada ETL
```

### Paso 4: Consultar con Athena
Usa el script `athena_queries.py` para:
- Agregar energ√≠a vendida por tipo
- Obtener los principales clientes por consumo
- Evaluar desempe√±o de proveedores

### Paso 5: Redshift Spectrum
```sql
CREATE EXTERNAL SCHEMA curated
FROM data catalog
DATABASE 'energy_trading_curated_db'
IAM_ROLE 'arn:aws:iam::ACCOUNT_ID:role/RedshiftSpectrumRole'
CREATE EXTERNAL DATABASE IF NOT EXISTS;

SELECT * FROM curated.clients LIMIT 10;
```

## ‚úÖ Funcionalidades Implementadas
- ‚úÖ Estructura en S3 con m√∫ltiples zonas y datos particionados
- ‚úÖ Jobs de Glue con conversi√≥n de formato y particionado
- ‚úÖ Crawlers y Cat√°logo de Glue por zona (`raw`, `processed`, `curated`)
- ‚úÖ Integraci√≥n con Athena v√≠a Python (`boto3`)
- ‚úÖ Integraci√≥n con Redshift Spectrum
- ‚úÖ Infraestructura como c√≥digo con CloudFormation modular
- ‚úÖ Roles IAM y pol√≠ticas de acceso refinado a S3

## üîí Seguridad y Gobernanza
- Encriptaci√≥n en S3 (por defecto)
- Control de acceso granular con Lake Formation
- Roles IAM para Glue, Redshift y Athena
- Particionado de datos para reducir costos de consulta

## üß™ Pruebas
Validaciones aplicadas en scripts de transformaci√≥n:
- Eliminaci√≥n de registros nulos
- Deduplicaci√≥n por campos clave
- Creaci√≥n y verificaci√≥n de particiones en S3

## üîç Pruebas Unitarias

El proyecto incluye una carpeta test/ que agrupa pruebas automatizadas dise√±adas como soporte adicional para validar funcionalidades cr√≠ticas del pipeline:

- test_athena_queries.py: Simula la ejecuci√≥n de una consulta en Athena usando boto3 con mocks, validando el flujo completo sin depender de la nube.
- test_data_validation.py: Prueba la validaci√≥n de esquemas en DataFrames, asegurando que cumplan con las columnas esperadas.
- test_etl_jobs.py: Valida la creaci√≥n de un DataFrame con Spark y su estructura m√≠nima como parte de una transformaci√≥n.
- conftest.py: Define fixtures reutilizables con datos de prueba.
  
Estas pruebas no son requeridas en el despliegue por CloudFormation, pero aportan valor t√©cnico y demuestran buenas pr√°cticas de calidad y validaci√≥n del c√≥digo.

> Nota: Se elimin√≥ test_utils.py por no aportar validaci√≥n funcional real.

## üì∑ Validaci√≥n, Evidencias y Mejoras Pendientes

- Todo el pipeline fue probado completamente en AWS.  
- La infraestructura fue desplegada exitosamente usando plantillas de **AWS CloudFormation**, incluyendo m√≥dulos para S3, Glue, Lake Formation y Redshift.
- Se valid√≥ que cada componente se ejecutara correctamente:
  - Glue Jobs completaron con √©xito.
  - Athena ejecut√≥ consultas sobre particiones correctamente.
  - Redshift Spectrum pudo consultar datos desde la zona curated.
- Se adjuntan im√°genes de las ejecuciones en AWS Console en el documento entregado.
- 
- **Mejoras pendientes**:
  - Agregar control de duplicados en las zonas raw/processed.
  - Optimizar validaciones de calidad de datos.
  - Automatizar ejecuci√≥n de pruebas unitarias para los ETL scripts.
  - Se inici√≥ el proceso para conectar el repositorio de **GitHub con AWS** con el objetivo de automatizar despliegues directamente desde el c√≥digo fuente.  
  Aunque la integraci√≥n no se complet√≥, se document√≥ la intenci√≥n y se dej√≥ preparado el entorno para su futura implementaci√≥n como una mejora de CI/CD.

  ## üß™ Carpeta `experimental/` (antes `src/`)

Durante el desarrollo del proyecto, se cre√≥ una carpeta con el nombre original `src/` que conten√≠a scripts, transformaciones y utilidades desarrolladas como parte del proceso exploratorio y t√©cnico.

Esta carpeta fue renombrada como `experimental/` para reflejar su prop√≥sito real: agrupar componentes que **no fueron utilizados directamente en el pipeline desplegado con CloudFormation**, pero que sirvieron para:

- Probar transformaciones modulares por dataset (`transform_customers.py`, `transform_providers.py`, etc.).
- Generar datos de ejemplo (`generate_sample_data.py`).
- Explorar consultas y vistas SQL (`analysis_queries.sql`, `create_views.sql`).
- Crear utilidades reusables para fechas, logs y S3 (`utils/`).

Debido a que el pipeline final fue **validado directamente en AWS como infraestructura desplegada por CloudFormation**, estos componentes **no fueron necesarios en la ejecuci√≥n oficial**, pero reflejan el an√°lisis previo y el soporte t√©cnico realizado durante el desarrollo del proyecto.

Se decidi√≥ conservar esta carpeta para documentar el proceso completo y mantener trazabilidad del trabajo exploratorio realizado.

## üìÑ Licencia
Licencia MIT
