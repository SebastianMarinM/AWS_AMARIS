
# âš¡ Proyecto Data Lake para ComercializaciÃ³n de EnergÃ­a

## ğŸ“Œ DescripciÃ³n General
Este proyecto implementa una arquitectura completa de Data Lake para una empresa comercializadora de energÃ­a utilizando AWS. Se ingieren archivos CSV desde sistemas operativos (clientes, proveedores, transacciones), se procesan con AWS Glue, se almacenan en S3 en mÃºltiples zonas, se consultan mediante Athena y Redshift Spectrum, y se aplica gobernanza de datos con Lake Formation.

## ğŸ“ Arquitectura
El pipeline sigue una arquitectura en capas:

- **Zona Raw**: Archivos CSV originales ingeridos desde los sistemas fuente, particionados por fecha de carga.
- **Zona Procesada**: Datos convertidos a formato Parquet con validaciÃ³n bÃ¡sica y particionados por aÃ±o/mes/dÃ­a.
- **Zona Curada**: Conjuntos de datos limpios y deduplicados optimizados para analÃ­tica.
- **Acceso AnalÃ­tico**:
  - **Amazon Athena**: Consultas SQL directas sobre datos en S3.
  - **Amazon Redshift Spectrum**: Consultas externas sobre datos curados vÃ­a Glue Catalog.
  
## ğŸ›  TecnologÃ­as Utilizadas
- `Amazon S3`: Almacenamiento del Data Lake
- `AWS Glue`: ETL (Jobs + Crawlers + CatÃ¡logo)
- `Amazon Athena`: Motor de consultas serverless sobre S3
- `Amazon Redshift`: Data Warehouse + Spectrum
- `AWS Lake Formation`: Gobernanza de datos y permisos
- `AWS CloudFormation`: Infraestructura como CÃ³digo (IaC)
- `Python (boto3, pandas)`: OrquestaciÃ³n y scripts analÃ­ticos

## ğŸ“ Estructura del Proyecto
```
AWS_AMARIS/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Archivos CSV originales
â”‚ â”œâ”€â”€ processed/ # Datos Parquet transformados
â”‚ â””â”€â”€ curated/ # Datos listos para anÃ¡lisis
â”œâ”€â”€ docs/ # DocumentaciÃ³n y diagramas
â”‚ â”œâ”€â”€ ARCHITECTURE.md
â”‚ â”œâ”€â”€ QUICKSTART.md
â”‚ â””â”€â”€ technical_documentation.md
â”œâ”€â”€ experimental/ # Scripts no usados en el pipeline final
â”‚ â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ sql/
â”œâ”€â”€ infrastructure/
â”‚ â”œâ”€â”€ modules/ # Plantillas CloudFormation por servicio
â”‚ â””â”€â”€ environments/dev/ # Stack principal
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ python/ # ETL oficial validado en AWS
â”‚ â”œâ”€â”€ raw_to_processed.py
â”‚ â”œâ”€â”€ processed_to_curated.py
â”‚ â””â”€â”€ athena_queries.py
â”œâ”€â”€ test/ # Scripts de prueba y validaciÃ³n
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

## ğŸš€ GuÃ­a de Despliegue

### Paso 1: Desplegar Infraestructura
```bash
# Buckets S3
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/s3.yaml --stack-name datalake-s3

# Lake Formation y permisos
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/lakeformation.yaml --stack-name datalake-lakeformation

# Recursos de Glue (jobs, crawlers, roles)
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/glue.yaml --stack-name datalake-glue

# ClÃºster Redshift
aws cloudformation deploy --template-file infrastructure/cloudformation/modules/redshift.yaml --stack-name datalake-redshift
```

### Paso 2: Ejecutar ETL Jobs
Puedes ejecutarlos desde la consola de AWS o mediante trigger:
- `raw_to_processed.py`: Convierte CSV a Parquet y particiona por fecha.
- `processed_to_curated.py`: Limpia, deduplica y escribe datos curados.

### Paso 3: Actualizar CatÃ¡logo de Glue
```bash
# Ejecutar crawlers o configurar triggers luego de cada ETL
```

### Paso 4: Consultar con Athena
Usa el script `athena_queries.py` para:
- Agregar energÃ­a vendida por tipo
- Obtener los principales clientes por consumo
- Evaluar desempeÃ±o de proveedores

### Paso 5: Redshift Spectrum 
```sql
CREATE EXTERNAL SCHEMA curated
FROM data catalog
DATABASE 'energy_trading_curated_db'
IAM_ROLE 'arn:aws:iam::ACCOUNT_ID:role/RedshiftSpectrumRole'
CREATE EXTERNAL DATABASE IF NOT EXISTS;

-- Consulta de ejemplo
SELECT * FROM curated.clients LIMIT 10;
```

## âœ… Funcionalidades Implementadas
- âœ… Estructura en S3 con mÃºltiples zonas y datos particionados
- âœ… Jobs de Glue con conversiÃ³n de formato y particionado
- âœ… Crawlers y CatÃ¡logo de Glue por zona (`raw`, `processed`, `curated`)
- âœ… IntegraciÃ³n con Athena vÃ­a Python (`boto3`)
- âœ… IntegraciÃ³n con Redshift Spectrum
- âœ… Infraestructura como cÃ³digo con CloudFormation modular
- âœ… Roles IAM y polÃ­ticas de acceso refinado a S3

## ğŸ”’ Seguridad y Gobernanza
- EncriptaciÃ³n en S3 (por defecto)
- Control de acceso granular con Lake Formation
- Roles IAM para Glue, Redshift y Athena
- Particionado de datos para reducir costos de consulta

## ğŸ§ª Pruebas
Validaciones aplicadas en scripts de transformaciÃ³n:
- EliminaciÃ³n de registros nulos
- DeduplicaciÃ³n por campos clave
- CreaciÃ³n y verificaciÃ³n de particiones en S3

## ğŸ“· ValidaciÃ³n, Evidencias y Mejoras Pendientes

- Todo el pipeline fue probado completamente en AWS.  
- La infraestructura fue desplegada exitosamente usando plantillas de **AWS CloudFormation**, incluyendo mÃ³dulos para S3, Glue, Lake Formation y Redshift.
- Se validÃ³ que cada componente se ejecutara correctamente:
  - Glue Jobs completaron con Ã©xito.
  - Athena ejecutÃ³ consultas sobre particiones correctamente.
  - Redshift Spectrum pudo consultar datos desde la zona `curated`.
- Se adjuntan imÃ¡genes de las ejecuciones en AWS Console en el documento entregado.
- 
- **Mejoras pendientes**:
  - Agregar control de duplicados en las zonas raw/processed.
  - Optimizar validaciones de calidad de datos.
  - Automatizar ejecuciÃ³n de pruebas unitarias para los ETL scripts.
  - Se iniciÃ³ el proceso para conectar el repositorio de **GitHub con AWS** con el objetivo de automatizar despliegues directamente desde el cÃ³digo fuente.  
  Aunque la integraciÃ³n no se completÃ³, se documentÃ³ la intenciÃ³n y se dejÃ³ preparado el entorno para su futura implementaciÃ³n como una mejora de CI/CD.


  ## ğŸ§ª Carpeta `experimental/` (antes `src/`)

Durante el desarrollo del proyecto, se creÃ³ una carpeta con el nombre original `src/` que contenÃ­a scripts, transformaciones y utilidades desarrolladas como parte del proceso exploratorio y tÃ©cnico.

Esta carpeta fue renombrada como `experimental/` para reflejar su propÃ³sito real: agrupar componentes que **no fueron utilizados directamente en el pipeline desplegado con CloudFormation**, pero que sirvieron para:

- Probar transformaciones modulares por dataset (`transform_customers.py`, `transform_providers.py`, etc.).
- Generar datos de ejemplo (`generate_sample_data.py`).
- Explorar consultas y vistas SQL (`analysis_queries.sql`, `create_views.sql`).
- Crear utilidades reusables para fechas, logs y S3 (`utils/`).

Debido a que el pipeline final fue **validado directamente en AWS como infraestructura desplegada por CloudFormation**, estos componentes **no fueron necesarios en la ejecuciÃ³n oficial**, pero reflejan el anÃ¡lisis previo y el soporte tÃ©cnico realizado durante el desarrollo del proyecto.

Se decidiÃ³ conservar esta carpeta para documentar el proceso completo y mantener trazabilidad del trabajo exploratorio realizado.


## ğŸ“„ Licencia
Licencia MIT

